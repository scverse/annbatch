{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Quickstart `annbatch`"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This notebook will walk you through the following steps:\n",
                "1. How to convert an existing collection of `anndata` files into a shuffled, zarr-based, collection of `anndata` datasets\n",
                "2. How to load the converted collection using `annbatch`\n",
                "3. Extend an existing collection with new `anndata` datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "tags": [
                    "hide-output"
                ]
            },
            "outputs": [],
            "source": [
                "# !pip install annbatch[zarrs, torch]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "tags": [
                    "hide-output"
                ]
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "zsh:1: command not found: wget\n",
                        "zsh:1: command not found: wget\n"
                    ]
                }
            ],
            "source": [
                "# Download two example datasets from CELLxGENE\n",
                "!wget https://datasets.cellxgene.cziscience.com/866d7d5e-436b-4dbd-b7c1-7696487d452e.h5ad\n",
                "!wget https://datasets.cellxgene.cziscience.com/f81463b8-4986-4904-a0ea-20ff02cbb317.h5ad"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**IMPORTANT**: Configure zarrs\n",
                "\n",
                "This step is both required for converting existing `anndata` files into a performant, shuffled collection of datasets for mini batch loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "tags": [
                    "hide-output"
                ]
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "<donfig.config_obj.ConfigSet at 0x10837ba70>"
                        ]
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import zarr\n",
                "\n",
                "zarr.config.set({\"codec_pipeline.path\": \"zarrs.ZarrsCodecPipeline\"})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "import warnings\n",
                "\n",
                "# Suppress zarr vlen-utf8 codec warnings\n",
                "warnings.filterwarnings(\n",
                "    \"ignore\",\n",
                "    message=\"The codec `vlen-utf8` is currently not part in the Zarr format 3 specification.*\",\n",
                "    category=UserWarning,\n",
                "    module=\"zarr.codecs.vlen_utf8\",\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Converting existing `anndata` files into a shuffled collection"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The conversion code will take care of the following things:\n",
                "* Align (outer join) the gene spaces across all datasets listed in `adata_paths`\n",
                "  * The gene spaces are outer-joined based on the gene names provided in the `var_names` field of the individual `AnnData` objects.\n",
                "  * If you want to subset to specific gene space, you can provide a list of gene names via the `var_subset` parameter.\n",
                "* Shuffle the cells across all datasets (this works on larger than memory datasets as well).\n",
                "  * This is important for block-wise shuffling during data loading.\n",
                "* Shuffle the input files across multiple output datasets:\n",
                "  * The size of each individual output dataset can be controlled via the `n_obs_per_dataset` parameter.\n",
                "  * We recommend to choose a dataset size that comfortably fits into system memory.\n",
                "\n",
                "\n",
                "You can apply custom data transformations to each input h5ad file by supplying a `load_adata` function to `DatasetCollection.add`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {
                "tags": [
                    "hide-output"
                ]
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "checking for mismatched keys: 100%|██████████| 2/2 [00:01<00:00,  1.73it/s]\n",
                        "/Users/ilangold/Projects/Theis/annbatch/src/annbatch/io.py:507: UserWarning: Found layers keys ['soupX'] not present in all anndatas ['866d7d5e-436b-4dbd-b7c1-7696487d452e.h5ad', 'f81463b8-4986-4904-a0ea-20ff02cbb317.h5ad'], consider stopping and using the `load_adata` argument to alter layers accordingly.\n",
                        "  _check_for_mismatched_keys(adata_paths)\n",
                        "/Users/ilangold/Projects/Theis/annbatch/src/annbatch/io.py:507: UserWarning: Found obs keys ['Eye', 'alignment_software', 'nCount_RNA', 'Region', 'donor_age', 'Post_mortemtime', 'reference_genome', 'library_id_repository', 'sequenced_fragment', 'donor_cause_of_death', 'sampleid', 'sequencing_platform', 'sample_id', 'nFeature_RNA', 'library_id', 'gene_annotation_version', 'Study', 'sample_collection_year', 'percent.mt', 'institute', 'Developmental', 'tissue_source', 'PMT_in_hrs', 'tissue_handling_interval', 'sample_collection_method', 'intronic_reads_counted', 'sample_source', 'leiden_scVI', 'library_starting_quantity', 'library_uuid', 'sample_uuid', 'sample_derivation_process', 'suspension_dissociation_reagent', 'donor_BMI_at_collection', 'mapped_reference_annotation', 'suspension_dissociation_time'] not present in all anndatas ['866d7d5e-436b-4dbd-b7c1-7696487d452e.h5ad', 'f81463b8-4986-4904-a0ea-20ff02cbb317.h5ad'], consider stopping and using the `load_adata` argument to alter obs accordingly.\n",
                        "  _check_for_mismatched_keys(adata_paths)\n",
                        "loading: 2it [00:00,  2.26it/s]\n",
                        "processing chunks:   0%|          | 0/1 [00:00<?, ?it/s]/Users/ilangold/Projects/Theis/annbatch/venv/lib/python3.12/site-packages/zarr/api/asynchronous.py:247: ZarrUserWarning: Consolidated metadata is currently not part in the Zarr format 3 specification. It may not be supported by other zarr implementations and may change in the future.\n",
                        "  warnings.warn(\n",
                        "processing chunks: 100%|██████████| 1/1 [00:24<00:00, 24.02s/it]\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "<annbatch.io.DatasetCollection at 0x12e174fb0>"
                        ]
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import anndata as ad\n",
                "from annbatch import DatasetCollection\n",
                "\n",
                "\n",
                "# For CELLxGENE data, the raw counts can either be found under .raw.X or under .X (if .raw is not supplied).\n",
                "# To have a store that only contains raw counts, we can write the following load_adata function\n",
                "def read_lazy_x_and_obs_only(path) -> ad.AnnData:\n",
                "    \"\"\"Custom load function to only load raw counts from CxG data.\"\"\"\n",
                "    # IMPORTANT: Large data should always be loaded lazily to reduce the memory footprint\n",
                "    adata_ = ad.experimental.read_lazy(path)\n",
                "    if adata_.raw is not None:\n",
                "        x = adata_.raw.X\n",
                "        var = adata_.raw.var\n",
                "    else:\n",
                "        x = adata_.X\n",
                "        var = adata_.var\n",
                "\n",
                "    return ad.AnnData(\n",
                "        X=x,\n",
                "        obs=adata_.obs.to_memory(),\n",
                "        var=var.to_memory(),\n",
                "    )\n",
                "\n",
                "\n",
                "collection = DatasetCollection(zarr.open(\"annbatch_collection\"))\n",
                "collection.add(\n",
                "    # List all the h5ad files you want to include in the collection\n",
                "    adata_paths=[\"866d7d5e-436b-4dbd-b7c1-7696487d452e.h5ad\", \"f81463b8-4986-4904-a0ea-20ff02cbb317.h5ad\"],\n",
                "    # Path to store the output collection\n",
                "    shuffle=True,  # Whether to pre-shuffle the cells of the collection\n",
                "    n_obs_per_dataset=2_097_152,  # Number of cells per dataset shard\n",
                "    var_subset=None,  # Optionally subset the collection to a specific gene space\n",
                "    load_adata=read_lazy_x_and_obs_only,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data loading example"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "\n",
                "COLLECTION_PATH = Path(\"annbatch_collection/\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {
                "tags": [
                    "hide-output"
                ]
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "<annbatch.loader.Loader at 0x12c2bfa40>"
                        ]
                    },
                    "execution_count": 14,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import anndata as ad\n",
                "\n",
                "from annbatch import Loader\n",
                "\n",
                "ds = Loader(\n",
                "    batch_size=4096,  # Total number of obs per yielded batch\n",
                "    chunk_size=256,  # Number of obs to load from disk contiguously - default settings should work well\n",
                "    preload_nchunks=32,  # Number of chunks to preload + shuffle - default settings should work well\n",
                "    preload_to_gpu=False,\n",
                "    # If True, preloaded chunks are moved to GPU memory via `cupy`, which can put more pressure on GPU memory but will accelerate loading ~20%\n",
                "    to_torch=True,\n",
                ")\n",
                "\n",
                "# Add in the shuffled data that should be used for training\n",
                "ds.add_collection(collection)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**IMPORTANT:**\n",
                "* The `Loader` yields batches of sparse tensors.\n",
                "* The conversion to dense tensors should be done on the GPU, as shown in the example below.\n",
                "  * First call `.cuda()` and then `.to_dense()`\n",
                "  * E.g. `x = x.cuda().to_dense()`\n",
                "  * This is significantly faster than doing the dense conversion on the CPU.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {
                "tags": [
                    "hide-output"
                ]
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  0%|          | 42/171792 [00:10<12:13:16,  3.90it/s]\n"
                    ]
                }
            ],
            "source": [
                "# Iterate over dataloader\n",
                "import tqdm\n",
                "\n",
                "for batch in tqdm.tqdm(ds):\n",
                "    x, obs = batch[\"data\"], batch[\"labels\"][\"cell_type\"]\n",
                "    # Important: Convert to dense on GPU\n",
                "    x = x.cuda().to_dense()\n",
                "    # Feed data into your model\n",
                "    ..."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Optional: Extend an existing collection with a new dataset"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "You might want to extend an existing pre-shuffled collection with a new dataset.\n",
                "This can be done using the `add_to_collection` function.\n",
                "\n",
                "This function will take care of shuffling the new dataset into the existing collection without having to re-shuffle the entire collection."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {
                "tags": [
                    "hide-output"
                ]
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "checking for mismatched keys: 100%|██████████| 1/1 [00:00<00:00,  2.09it/s]\n",
                        "loading: 1it [00:10, 10.77s/it]\n",
                        "checking for mismatched keys: 100%|██████████| 2/2 [00:01<00:00,  1.18it/s]\n",
                        "/Users/ilangold/Projects/Theis/annbatch/src/annbatch/io.py:613: UserWarning: Found obs keys ['sample_source', 'leiden_scVI', 'library_starting_quantity', 'library_uuid', 'sample_uuid', 'sample_derivation_process', 'suspension_dissociation_reagent', 'donor_BMI_at_collection', 'mapped_reference_annotation', 'suspension_dissociation_time'] not present in all anndatas [AnnData object with n_obs × n_vars = 99457 × 35475\n",
                        "    obs: 'reference_genome', 'gene_annotation_version', 'alignment_software', 'intronic_reads_counted', 'donor_id', 'donor_age', 'self_reported_ethnicity_ontology_term_id', 'donor_cause_of_death', 'donor_living_at_sample_collection', 'sample_id', 'sample_preservation_method', 'tissue_ontology_term_id', 'development_stage_ontology_term_id', 'sample_collection_method', 'tissue_source', 'tissue_type', 'sample_collection_year', 'suspension_derivation_process', 'suspension_uuid', 'suspension_type', 'tissue_handling_interval', 'library_id', 'assay_ontology_term_id', 'sequenced_fragment', 'institute', 'library_id_repository', 'sequencing_platform', 'is_primary_data', 'cell_type_ontology_term_id', 'author_cell_type', 'disease_ontology_term_id', 'reported_diseases', 'sex_ontology_term_id', 'nCount_RNA', 'nFeature_RNA', 'percent.mt', 'Study', 'Developmental', 'Post_mortemtime', 'PMT_in_hrs', 'Eye', 'Region', 'sampleid', 'cell_type', 'assay', 'disease', 'sex', 'tissue', 'self_reported_ethnicity', 'development_stage', 'observation_joinid', 'src_path'\n",
                        "    var: 'feature_name', 'feature_reference', 'feature_biotype', 'feature_length', 'feature_type', <Group file://annbatch_collection/dataset_0>], consider stopping and using the `load_adata` argument to alter obs accordingly.\n",
                        "  _check_for_mismatched_keys([adata_concat] + [self._group[k] for k in self._dataset_keys])\n",
                        "processing chunks:   0%|          | 0/1 [00:00<?, ?it/s]/Users/ilangold/Projects/Theis/annbatch/venv/lib/python3.12/site-packages/anndata/_core/anndata.py:1806: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
                        "  utils.warn_names_duplicates(\"obs\")\n",
                        "/Users/ilangold/Projects/Theis/annbatch/venv/lib/python3.12/site-packages/zarr/api/asynchronous.py:247: ZarrUserWarning: Consolidated metadata is currently not part in the Zarr format 3 specification. It may not be supported by other zarr implementations and may change in the future.\n",
                        "  warnings.warn(\n",
                        "processing chunks: 100%|██████████| 1/1 [00:21<00:00, 21.74s/it]\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "<annbatch.io.DatasetCollection at 0x12e174fb0>"
                        ]
                    },
                    "execution_count": 19,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "def read_x_and_obs_only(path) -> ad.AnnData:\n",
                "    \"\"\"Custom load function to only load raw counts from CxG data.\"\"\"\n",
                "    # As it's only a small dataset, we can load the full dataset into memory to speed up computations\n",
                "    adata_ = ad.read_h5ad(path)  # Replace with ad.experimental.read_lazy if data does not fit into memory anymore\n",
                "    if adata_.raw is not None:\n",
                "        x = adata_.raw.X\n",
                "        var = adata_.raw.var\n",
                "    else:\n",
                "        x = adata_.X\n",
                "        var = adata_.var\n",
                "\n",
                "    return ad.AnnData(X=x, obs=adata_.obs, var=var)\n",
                "\n",
                "\n",
                "collection.add(\n",
                "    adata_paths=[\n",
                "        \"866d7d5e-436b-4dbd-b7c1-7696487d452e.h5ad\",\n",
                "    ],\n",
                "    load_adata=read_x_and_obs_only,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
