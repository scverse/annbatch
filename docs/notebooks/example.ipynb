{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart `annbatch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will walk you through the following steps:\n",
    "1. How to convert an existing collection of `anndata` files into a shuffled, zarr-based, collection of `anndata` datasets\n",
    "2. How to load the converted collection using `annbatch`\n",
    "3. Extend an existing collection with new `anndata` datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# !pip install annbatch[zarrs, torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n",
      "zsh:1: command not found: wget\n"
     ]
    }
   ],
   "source": [
    "# Download two example datasets from CELLxGENE\n",
    "!wget https://datasets.cellxgene.cziscience.com/866d7d5e-436b-4dbd-b7c1-7696487d452e.h5ad\n",
    "!wget https://datasets.cellxgene.cziscience.com/f81463b8-4986-4904-a0ea-20ff02cbb317.h5ad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: Configure zarrs\n",
    "\n",
    "This step is both required for converting existing `anndata` files into a performant, shuffled collection of datasets for mini batch loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<donfig.config_obj.ConfigSet at 0x127a5ed80>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import zarr\n",
    "\n",
    "zarr.config.set({\"codec_pipeline.path\": \"zarrs.ZarrsCodecPipeline\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress zarr vlen-utf8 codec warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"The codec `vlen-utf8` is currently not part in the Zarr format 3 specification.*\",\n",
    "    category=UserWarning,\n",
    "    module=\"zarr.codecs.vlen_utf8\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting existing `anndata` files into a shuffled collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conversion code will take care of the following things:\n",
    "* Align (outer join) the gene spaces across all datasets listed in `adata_paths`\n",
    "  * The gene spaces are outer-joined based on the gene names provided in the `var_names` field of the individual `AnnData` objects.\n",
    "  * If you want to subset to specific gene space, you can provide a list of gene names via the `var_subset` parameter.\n",
    "* Shuffle the cells across all datasets (this works on larger than memory datasets as well).\n",
    "  * This is important for block-wise shuffling during data loading.\n",
    "* Shuffle the input files across multiple output datasets:\n",
    "  * The size of each individual output dataset can be controlled via the `n_obs_per_dataset` parameter.\n",
    "  * We recommend to choose a dataset size that comfortably fits into system memory.\n",
    "\n",
    "\n",
    "You can apply custom data transformations to each input h5ad file by supplying a `load_adata` function to `DatasetCollection.add`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ilangold/Projects/Theis/annbatch/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "checking for mismatched keys: 100%|██████████| 2/2 [00:00<00:00,  2.19it/s]\n",
      "loading: 2it [00:00,  2.19it/s]\n",
      "processing chunks:   0%|          | 0/1 [00:00<?, ?it/s]/Users/ilangold/Projects/Theis/annbatch/venv/lib/python3.12/site-packages/zarr/api/asynchronous.py:247: ZarrUserWarning: Consolidated metadata is currently not part in the Zarr format 3 specification. It may not be supported by other zarr implementations and may change in the future.\n",
      "  warnings.warn(\n",
      "processing chunks: 100%|██████████| 1/1 [00:21<00:00, 21.20s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<annbatch.io.DatasetCollection at 0x15ff0f8f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import anndata as ad\n",
    "from annbatch import DatasetCollection\n",
    "\n",
    "# let's write out only shared colunms - otherwise DatasetCollection will warn about all the columns we are missing for good reason - mismatched columns can lead to unexpected data and missing values.\n",
    "shared_columns = ad.experimental.read_lazy(\"866d7d5e-436b-4dbd-b7c1-7696487d452e.h5ad\").obs.columns.intersection(\n",
    "    ad.experimental.read_lazy(\"f81463b8-4986-4904-a0ea-20ff02cbb317.h5ad\").obs.columns\n",
    ")\n",
    "\n",
    "\n",
    "# For CELLxGENE data, the raw counts can either be found under .raw.X or under .X (if .raw is not supplied).\n",
    "# To have a store that only contains raw counts, we can write the following load_adata function\n",
    "def read_lazy_x_and_obs_only(path) -> ad.AnnData:\n",
    "    \"\"\"Custom load function to only load raw counts from CxG data.\"\"\"\n",
    "    # IMPORTANT: Large data should always be loaded lazily to reduce the memory footprint\n",
    "    adata_ = ad.experimental.read_lazy(path)\n",
    "    if adata_.raw is not None:\n",
    "        x = adata_.raw.X\n",
    "        var = adata_.raw.var\n",
    "    else:\n",
    "        x = adata_.X\n",
    "        var = adata_.var\n",
    "\n",
    "    return ad.AnnData(\n",
    "        X=x,\n",
    "        obs=adata_.obs.to_memory()[shared_columns],\n",
    "        var=var.to_memory(),\n",
    "    )\n",
    "\n",
    "\n",
    "collection = DatasetCollection(zarr.open(\"annbatch_collection\", mode=\"w\"))\n",
    "collection.add_adatas(\n",
    "    # List all the h5ad files you want to include in the collection\n",
    "    adata_paths=[\"866d7d5e-436b-4dbd-b7c1-7696487d452e.h5ad\", \"f81463b8-4986-4904-a0ea-20ff02cbb317.h5ad\"],\n",
    "    # Path to store the output collection\n",
    "    shuffle=True,  # Whether to pre-shuffle the cells of the collection\n",
    "    n_obs_per_dataset=2_097_152,  # Number of cells per dataset shard, this number is much higher than available in these datasets but is generally a good target\n",
    "    var_subset=None,  # Optionally subset the collection to a specific gene space\n",
    "    load_adata=read_lazy_x_and_obs_only,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create our data loader with the desired arguments.\n",
    "\n",
    "**WARNING**: Without `load_adata` argument in `use_collection`, the *entire* `obs` will be loaded and yielded, degrading performance.  It is highly advised to use this argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<annbatch.loader.Loader at 0x1619de480>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import anndata as ad\n",
    "\n",
    "from annbatch import Loader\n",
    "\n",
    "\n",
    "def _load_adata(g: zarr.Group) -> ad.AnnData:\n",
    "    return ad.AnnData(X=ad.io.sparse_dataset(g[\"X\"]), obs=ad.experimental.read_lazy(g).obs[[\"cell_type\"]].to_memory())\n",
    "\n",
    "\n",
    "ds = Loader(\n",
    "    batch_size=4096,  # Total number of obs per yielded batch\n",
    "    chunk_size=256,  # Number of obs to load from disk contiguously - default settings should work well\n",
    "    preload_nchunks=32,  # Number of chunks to preload + shuffle - default settings should work well\n",
    "    # If True, preloaded chunks are moved to GPU memory via `cupy`, which can put more pressure on GPU memory but will accelerate loading ~20%\n",
    "    preload_to_gpu=False,\n",
    "    to_torch=True,\n",
    ")\n",
    "\n",
    "# Add in the shuffled data that should be used for training.\n",
    "ds.use_collection(collection, load_adata=_load_adata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT:**\n",
    "* The `Loader` yields batches of sparse tensors.\n",
    "* The conversion to dense tensors should be done on the GPU, as shown in the example below.\n",
    "  * First call `.cuda()` and then `.to_dense()`\n",
    "  * E.g. `x = x.cuda().to_dense()`\n",
    "  * This is significantly faster than doing the dense conversion on the CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 42/171792 [00:07<8:54:32,  5.36it/s] \n"
     ]
    }
   ],
   "source": [
    "# Iterate over dataloader\n",
    "import tqdm\n",
    "\n",
    "for batch in tqdm.tqdm(ds):\n",
    "    x, obs = batch[\"X\"], batch[\"obs\"][\"cell_type\"]\n",
    "    # Important: Convert to dense on GPU\n",
    "    x = x.cuda().to_dense()\n",
    "    # Feed data into your model\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Extend an existing collection with a new dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might want to extend an existing pre-shuffled collection with a new dataset.\n",
    "This can be done using the `add` method again.\n",
    "\n",
    "This function will take care of shuffling the new dataset into the existing collection without having to re-shuffle the entire collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checking for mismatched keys: 100%|██████████| 1/1 [00:00<00:00,  1.65it/s]\n",
      "loading: 1it [00:00,  1.77it/s]\n",
      "checking for mismatched keys: 100%|██████████| 2/2 [00:00<00:00, 13.66it/s]\n",
      "processing chunks:   0%|          | 0/1 [00:00<?, ?it/s]/Users/ilangold/Projects/Theis/annbatch/venv/lib/python3.12/site-packages/anndata/_core/anndata.py:1806: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"obs\")\n",
      "/Users/ilangold/Projects/Theis/annbatch/venv/lib/python3.12/site-packages/anndata/_core/anndata.py:1806: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"obs\")\n",
      "/Users/ilangold/Projects/Theis/annbatch/venv/lib/python3.12/site-packages/zarr/api/asynchronous.py:247: ZarrUserWarning: Consolidated metadata is currently not part in the Zarr format 3 specification. It may not be supported by other zarr implementations and may change in the future.\n",
      "  warnings.warn(\n",
      "processing chunks: 100%|██████████| 1/1 [00:21<00:00, 21.82s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<annbatch.io.DatasetCollection at 0x13d8afec0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.add_adatas(\n",
    "    adata_paths=[\n",
    "        \"866d7d5e-436b-4dbd-b7c1-7696487d452e.h5ad\",\n",
    "    ],\n",
    "    load_adata=read_lazy_x_and_obs_only,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
