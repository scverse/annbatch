{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart `annbatch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will walk you through the following steps:\n",
    "1. How to convert an existing collection of `anndata` files into a shuffled, zarr-based, collection of `anndata` datasets\n",
    "2. How to load the converted collection using `annbatch`\n",
    "3. Extend an existing collection with new `anndata` datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# !pip install annbatch[zarrs, torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-10-09 09:43:19--  https://datasets.cellxgene.cziscience.com/866d7d5e-436b-4dbd-b7c1-7696487d452e.h5ad\n",
      "Resolving datasets.cellxgene.cziscience.com (datasets.cellxgene.cziscience.com)... 18.64.79.73, 18.64.79.80, 18.64.79.109, ...\n",
      "Connecting to datasets.cellxgene.cziscience.com (datasets.cellxgene.cziscience.com)|18.64.79.73|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 773247972 (737M) [binary/octet-stream]\n",
      "Saving to: ‘866d7d5e-436b-4dbd-b7c1-7696487d452e.h5ad’\n",
      "\n",
      "866d7d5e-436b-4dbd- 100%[===================>] 737.43M   398MB/s    in 1.9s    \n",
      "\n",
      "2025-10-09 09:43:21 (398 MB/s) - ‘866d7d5e-436b-4dbd-b7c1-7696487d452e.h5ad’ saved [773247972/773247972]\n",
      "\n",
      "--2025-10-09 09:43:22--  https://datasets.cellxgene.cziscience.com/f81463b8-4986-4904-a0ea-20ff02cbb317.h5ad\n",
      "Resolving datasets.cellxgene.cziscience.com (datasets.cellxgene.cziscience.com)... 18.64.79.73, 18.64.79.80, 18.64.79.72, ...\n",
      "Connecting to datasets.cellxgene.cziscience.com (datasets.cellxgene.cziscience.com)|18.64.79.73|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1631759823 (1.5G) [binary/octet-stream]\n",
      "Saving to: ‘f81463b8-4986-4904-a0ea-20ff02cbb317.h5ad’\n",
      "\n",
      "f81463b8-4986-4904- 100%[===================>]   1.52G   425MB/s    in 3.9s    \n",
      "\n",
      "2025-10-09 09:43:26 (403 MB/s) - ‘f81463b8-4986-4904-a0ea-20ff02cbb317.h5ad’ saved [1631759823/1631759823]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download two example datasets from CELLxGENE\n",
    "!wget https://datasets.cellxgene.cziscience.com/866d7d5e-436b-4dbd-b7c1-7696487d452e.h5ad\n",
    "!wget https://datasets.cellxgene.cziscience.com/f81463b8-4986-4904-a0ea-20ff02cbb317.h5ad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: Configure zarrs\n",
    "\n",
    "This step is both required for converting existing `anndata` files into a performant, shuffled collection of datasets for mini batch loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<donfig.config_obj.ConfigSet at 0x7f203c0b6900>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import zarr\n",
    "import zarrs  # noqa\n",
    "\n",
    "zarr.config.set({\"codec_pipeline.path\": \"zarrs.ZarrsCodecPipeline\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress zarr vlen-utf8 codec warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"The codec `vlen-utf8` is currently not part in the Zarr format 3 specification.*\",\n",
    "    category=UserWarning,\n",
    "    module=\"zarr.codecs.vlen_utf8\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting existing `anndata` files into a shuffled collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conversion code will take care of the following things:\n",
    "* Align (outer join) the gene spaces across all datasets listed in `adata_paths`\n",
    "  * The gene spaces are outer-joined based on the gene names provided in the `var_names` field of the individual `AnnData` objects.\n",
    "  * If you want to subset to specific gene space, you can provide a list of gene names via the `var_subset` parameter.\n",
    "* Shuffle the cells across all datasets (this works on larger than memory datasets as well).\n",
    "  * This is important for block-wise shuffling during data loading.\n",
    "* Shuffle the input files across multiple output datasets:\n",
    "  * The size of each individual output dataset can be controlled via the `n_obs_per_dataset` parameter.\n",
    "  * We recommend to choose a dataset size that comfortably fits into system memory.\n",
    "\n",
    "\n",
    "You can apply custom data transformations to each input h5ad file by supplying a `load_adata` function to `create_anndata_collection`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "import anndata as ad\n",
    "from annbatch import create_anndata_collection\n",
    "\n",
    "\n",
    "# For CELLxGENE data, the raw counts can either be found under .raw.X or under .X (if .raw is not supplied).\n",
    "# To have a store that only contains raw counts, we can write the following load_adata function\n",
    "def read_lazy_x_and_obs_only(path) -> ad.AnnData:\n",
    "    \"\"\"Custom load function to only load raw counts from CxG data.\"\"\"\n",
    "    # IMPORTANT: Large data should always be loaded lazily to reduce the memory footprint\n",
    "    adata_ = ad.experimental.read_lazy(path)\n",
    "    if adata_.raw is not None:\n",
    "        x = adata_.raw.X\n",
    "        var = adata_.raw.var\n",
    "    else:\n",
    "        x = adata_.X\n",
    "        var = adata_.var\n",
    "\n",
    "    return ad.AnnData(\n",
    "        X=x,\n",
    "        obs=adata_.obs.to_memory(),\n",
    "        var=var.to_memory(),\n",
    "    )\n",
    "\n",
    "\n",
    "create_anndata_collection(\n",
    "    # List all the h5ad files you want to include in the collection\n",
    "    adata_paths=[\"866d7d5e-436b-4dbd-b7c1-7696487d452e.h5ad\", \"f81463b8-4986-4904-a0ea-20ff02cbb317.h5ad\"],\n",
    "    # Path to store the output collection\n",
    "    output_path=\"annbatch_collection\",\n",
    "    shuffle=True,  # Whether to pre-shuffle the cells of the collection\n",
    "    n_obs_per_dataset=2_097_152,  # Number of cells per dataset shard\n",
    "    var_subset=None,  # Optionally subset the collection to a specific gene space\n",
    "    should_denseify=False,\n",
    "    load_adata=read_lazy_x_and_obs_only,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "COLLECTION_PATH = Path(\"annbatch_collection/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<annbatch.sparse.ZarrSparseDataset at 0x7f1ed87a88c0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import anndata as ad\n",
    "\n",
    "from annbatch import ZarrSparseDataset\n",
    "\n",
    "ds = ZarrSparseDataset(\n",
    "    batch_size=4096,  # Total number of obs per yielded batch\n",
    "    chunk_size=256,  # Number of obs to load from disk contiguously - default settings should work well\n",
    "    preload_nchunks=32,  # Number of chunks to preload + shuffle - default settings should work well\n",
    "    preload_to_gpu=False,\n",
    "    # If True, preloaded chunks are moved to GPU memory via `cupy`, which can put more pressure on GPU memory but will accelerate loading ~20%\n",
    "    to_torch=True,\n",
    ")\n",
    "\n",
    "# Add dataset that should be used for training\n",
    "ds.add_anndatas(\n",
    "    [\n",
    "        ad.AnnData(\n",
    "            X=ad.io.sparse_dataset(zarr.open(p)[\"X\"]),\n",
    "            obs=ad.io.read_elem(zarr.open(p)[\"obs\"]),\n",
    "        )\n",
    "        for p in COLLECTION_PATH.glob(\"*.zarr\")\n",
    "    ],\n",
    "    obs_keys=\"cell_type\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT:**\n",
    "* The `ZarrSparseDataset` yields batches of sparse tensors.\n",
    "* The conversion to dense tensors should be done on the GPU, as shown in the example below.\n",
    "  * First call `.cuda()` and then `.to_dense()`\n",
    "  * E.g. `x = x.cuda().to_dense()`\n",
    "  * This is significantly faster than doing the dense conversion on the CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/171792 [00:00<?, ?it/s]/mnt/volume/arrayloaders/src/annbatch/utils.py:307: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "  tensor = torch.sparse_csr_tensor(\n",
      "  0%|          | 42/171792 [00:04<5:33:36,  8.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# Iterate over dataloader\n",
    "import tqdm\n",
    "\n",
    "for batch in tqdm.tqdm(ds):\n",
    "    x, obs = batch\n",
    "    # Important: Convert to dense on GPU\n",
    "    x = x.cuda().to_dense()\n",
    "    # Feed data into your model\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Extend an existing collection with a new dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might want to extend an existing pre-shuffled collection with a new dataset.\n",
    "This can be done using the `add_to_collection` function.\n",
    "\n",
    "This function will take care of shuffling the new dataset into the existing collection without having to re-shuffle the entire collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]/home/ubuntu/home_drive/volume/arrayloaders/venv/lib/python3.12/site-packages/anndata/_core/anndata.py:1791: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"obs\")\n",
      "/home/ubuntu/home_drive/volume/arrayloaders/venv/lib/python3.12/site-packages/anndata/_core/anndata.py:1791: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"obs\")\n",
      "/home/ubuntu/home_drive/volume/arrayloaders/venv/lib/python3.12/site-packages/anndata/_core/anndata.py:1791: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"obs\")\n",
      "/home/ubuntu/home_drive/volume/arrayloaders/venv/lib/python3.12/site-packages/zarr/api/asynchronous.py:244: ZarrUserWarning: Consolidated metadata is currently not part in the Zarr format 3 specification. It may not be supported by other zarr implementations and may change in the future.\n",
      "  warnings.warn(\n",
      "100%|██████████| 1/1 [00:30<00:00, 30.76s/it]\n"
     ]
    }
   ],
   "source": [
    "from annbatch import add_to_collection\n",
    "\n",
    "\n",
    "def read_x_and_obs_only(path) -> ad.AnnData:\n",
    "    \"\"\"Custom load function to only load raw counts from CxG data.\"\"\"\n",
    "    # As it's only a small dataset, we can load the full dataset into memory to speed up computations\n",
    "    adata_ = ad.read_h5ad(path)  # Replace with ad.experimental.read_lazy if data does not fit into memory anymore\n",
    "    if adata_.raw is not None:\n",
    "        x = adata_.raw.X\n",
    "        var = adata_.raw.var\n",
    "    else:\n",
    "        x = adata_.X\n",
    "        var = adata_.var\n",
    "\n",
    "    return ad.AnnData(X=x, obs=adata_.obs, var=var)\n",
    "\n",
    "\n",
    "add_to_collection(\n",
    "    adata_paths=[\n",
    "        \"866d7d5e-436b-4dbd-b7c1-7696487d452e.h5ad\",\n",
    "    ],\n",
    "    output_path=\"annbatch_collection\",\n",
    "    load_adata=read_x_and_obs_only,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
